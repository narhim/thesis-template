%!TEX root = ../Thesis.tex
\label{chap:con}
\section{Summary of Results}
Throughout these pages two studies on the veridicality of mood alternation and specificity have been presented and their combine results draw a fascinating but still incomplete picture.\\

First of all, annotations in both studies are similarly distributed: negatively skewed and the positive labels have higher frequencies than the negatives ones. Differences lay mainly on the use of the negative labels and the neutral label Uu (unknown). The first is easily explained by taking a simple look at both corpora: the pilot corpus has two mood alternation conditions (negation and possibility), whereas the main corpus comprises only the negative condition. As to the use of the label Uu is be explained by the same reasoning, since by having the possibility condition, which consists on adverbs like \textit{quiz√°s, probablemente, tal vez} (maybe, probably, perhaps); the probability of having more premise-hypohesis pairs increases greatly.\\

Secondly, results of both experiments show a persistent use of the label \textit{not a sentence}. As explained in Section \ref{sect:expdes}, this label was included to ensure the acceptability of pairs in the possibility condition, but still, as stated in Section \ref{subsect:piloverview}, it was used in all sort of pairs although never with a majority agreement. Furthermore, although the possibility condition was removed from the main study, the label was kept and explicitly discouraged in the instructions. Still, as shown in Section \ref{sect:nas}, the label was used and in a similar proportion with respect to the pilot study, even if with differences regarding its distribution across experimental conditions. It is difficult to point what exactly causes these results, but they do point to the difficulty of the task.\\

Thirdly, both studies have an inter-annotator agreement score that is not quite high, specially when compared to studies like \citet{sauri2009factbank} or \citet{ross2019well}. Nevertheless, the overall agreement for the pilot study ($AC_2=0.484$) is still within reach of the work of \citet{de2012did} ($\kappa = 0.53$), a main reference to this thesis, is also within the range of fair agreement $(0.41-0.60)$ \citep{shrout1998measurement}, and thus it is not considered as problematic. Difficulties arise when we look at the negation condition in the first experiment and at the main study. On the first one, agreement was lower than the overall score ($AC_2=0.388$) and drops to the range of slight agreement $(0.11-0.40)$, even if in the higher end. Although as stated in Section \ref{sect:pildisc}, it was expected that improvements in the experimental design and procedure would increase for the main study, results show the opposite: agreement decreases to $AC_2=0.114$, barely within the range of slight agreement. This clearly shows that the experiment design followed here is inherently harder than others \citep{pavlick2019inherent}. A more thorough discussion on agreement is presented below in Section \ref{sect:dis}.\\

Lastly, regarding the cumulative link mixed models (CLMMs), both show the uninformativeness of the specificity conditions and the most important differences between both studies lie on the variance of the random variables and on the significant effects yielded. The model for the first experiment shows considerable more variance for both annotator and pair than the main study's model. As to the significant effects, it is interesting that the first one does not have any for the negation condition but the main study's model does, even though it depends on the predictors chosen. These results suggest that there is a relevance to the veridicality of the negation condition and none to the specificity condition. Next a more thorough discussion on the results regarding agreement is presented.\\

\section{Disagreement}\label{sect:dis}
When gathering crowd or expert annotations it is often the case that their inter-annotator agreement score, in its different variants, is taken as a measure of how good the annotations are and thus when the expected level is not reached, different measures are taken to improve agreement. This is the path that was followed here, but given the results obtained a careful discussion on the topic is needed.\\

The first step is to understand what causes disagreement. \citet{uma2021learning} defines five sources: errors and interface problems, annotation scheme, ambiguity, item difficulty and subjectivity. None of these can be completely rejected in our case, but given the persistance of some agreement patterns seen in Section \ref{subsect:agrepat} and that the interface was a simple list, not all of the disagreement can be atributed to errors or interface problems.\\ 

As to the annotation scheme, there is definitely some improvements that could be implemented. Firstly, a stricter quality control could not just improve agreement but also yield stronger tendencies and therefore explanations. Initially, it was considered that training the annotators would prevent them from making natural annotations. But after encountering the work of \citet{nie2020can}, where they used carefully crafted training and testing that did not fully prevented disagreement, it is understood that training and testing could have been implemented, although it would have been a difficult implementation within the size of this corpus, the timeline given and external circumnstances. In addition, it is true that the criteria for the quality control implemented were defined as the annotations were encountered, rather than before running the study, and not with a lot of confidence. For example, it was clear that annotations done \textit{too fast} had to be rejected, but the problem was defining what exactly too fast meant and even more, what about annotations that took \textit{too long}? Another conflicting decision was determining when labels were misused. It was clear that if a set of annotations had been done relatively fast and had all, or almost all the same label they had to be rejected. But what if they were done in a \textit{timely manner} but still had the same label? The rejection criteria should have indeed been defined beforehand, but also some references in this matter were and are still needed. Lastly there was the problem of the instructions. Given the feedback received from the pilot study, the instructions were reviewed before running the main experiment, but seeing the results, it is not clear wether this helped or not.\\

Secondly, in terms of postprocessing there is certain confidence in the decisions made, but there is room for improvement. On the main study pairs which were labelled as \textit{not a sentence} were removed, but contrary to what was done in the pilot study, whole seeds were not removed. This was done to prevent losing too much data, but it is true that there is no evidence to support this decision. Furthermore, a constant question when analyzing the annotations was whether there are more filters that could be applied and how to measure their effectiveness. Thus it can be said that other filters could have improved the results.\\

Thirdly, given that already in the pilot study it was proven that the effect of mood alternation depends upon the matrix verbs and, as seen in Section \ref{subsect:mod}, that when not controlled it is difficult to fully analyze its effect, it is likely that a corpus balanced not just in terms of mood but also in terms of matrices could have provided more understandable results.\\

Lastly, after seeing the results of both studies and seeing the amount of different labels used it is clear that the assumptions must be revised. Specifically about the of negating the matrix verb. It was assumed that the negation of the matrix verb implied that the embedded predicate would be negated, even if not with complete certainty. But as seen in Section \ref{subsect:man}, this is not always the case. Thus it could have probably been better to first study the effect of negation and then of mood alternation.\\

Regarding the number of interpretations the relation between the different premises and their hypothesis can have, their ambiguity that is, the analysises presented in Section \ref{subsect:agrepat} and in Section \ref{subsect:man} have clearly shown that there are pairs for which there are several labels that could be assigned, the decision depending on factor like what one considers to be a universal truth. This makes disagreement nor an error nor a problem, but a natural ocurrence. Further more, as already mentioned in Section \ref{sect:verandfact}, the study of \citet{pavlick2019inherent} already proves the existence of an \textit{inherent} disagreement between annotators in a NLI task, that is, they show that there are pairs for which there is not a unique label or single truth, but rather a label split or two labels. The study of \citet{nie2020can} also supports the existence of inherent disagreement.\\

Also within this discussion of ambiguity there is one important topic that needs to be considered: the difference between the negation condition subset in the pilot study and the pilot subset in the main experiment. As seen in Section \ref{subsect:iaa}, specifically on Table \ref{tab:iaa}, the agreement in these subsets is radically different ($AC_2=0.388$ and $AC_2=0.080$) although they consist of the exact same pairs. It is obvious that a closer examination of these annotations is needed, but at first look, they can definitely be considered as evidence that there is not always a single label that can be assigned to these pairs.\\

There is one last issue that should be discussed within this ambiguity: the phenomena studied. The agreement scores obtained here are quite lower than previous work, but most of the previous studies mentioned here are factuality experiments and thus their corpora are not comprised of one or two phenomena, but on a set of phenomena whith different levels uncertainty, and therefore ambiguity, associated to them. But in our case we focus on two phenomena, mood alternation and specificity, and the first one, which has shown to have a greater effect than the second one, , as already state in Section \ref{sect:pildisc}, is a pragmatic phenomenon, and consequently a greater level uncertainty, and thus disagreement, is associated with it.\\

With respect to the fourth possible cause of disagreement, item difficulty, that is, how clear the interpretation of an item, it is here a clearly cause of disagreement. Firstly, as already mentioned in Section \ref{sect:pildisc}, one annotator of the pilot study mentioned that the task was difficult. Secondly, in Section \ref{subsect:agrepat} and in Section \ref{subsect:man} it was explained how there are different factors to be considered when given a factuality judgment. Lastly, previous work has shown NLI annotations to be difficult \citep{pavlick2019inherent,uma2021learning}. Thus it is clear that the task at hand is quite difficult. To reduce it, the above-mentioned improvements could help, but it is never going to disappear entirely.\\

As to the last factor for disagreement, subjectivity, it is also a cause for disagreement here. Although, to my knowledge, there is not previous work that support this as cause for disagreement in NLI annotations, the analysis presented in Section \ref{subsect:man} shows that world knowledge clearly influences speakers' judgments, thus making annotations dependent upon annotators' knowledge. This, together with the fact that there is a relation between mood alternation and sociolinguistics features \citep{waltermire2017mood}, at least partially explains why there is a such a big different in the $AC_2$ score between both studies, since in the first case most of the speakers were Spanish and on the second case their origin was much more diverse and thus their word knowledge was also much different.\\

Now that it has been explained what caused disagreement in these studies, the question is if an inter-annotator agreement scores, let it be $AC_2$ or any other, can actually reflect the nature and quality of the annotations gathered. The simple answer is no, at least not entirely. As stated in \citet{gwet2014handbook}, inter-annotator agreemenent reflects how much the annotations change when small adjustments in the annotators are made, that is, it is a measure of data reproducibility based on the individual annotators. But given that it has been proven that these annotations highly dependent on the speaker, measuring the reproducibility of the data based on such small variations is misguided and different evaluation scores are needed.\\

In this department, recent work has shown useful advances that could help better understand the annotations here gathered and increase their usability. For example, as already mentioned in Chapter \ref{chap:back}, \citet{pavlick2019inherent} computed Gaussian Mixture Model (GMM) for the annotations of each pair. We also have the work of \citet{nie2020can}, that collected 100 annotations for each pair of subsets from different NLI corpora, like MNLI, computed the labels' entropy for each pair and then examined the entropy distribution for each set of annotations; and the work of \citet{gordon2021disagreement}, which proposes an evaluation metric to be incorporated in machine learning algorithms that asks what proportion of the population the classifier agrees with instead of what proportion of ground truth labels the classifier agrees with. It is difficult to predict what these metrics could show about our corpora, but it definitely is a needed and insightful exploration. \\

Now that a proper discussion of the results has been given, the research questions of this thesis can be answered.\\

\section{Answer to Research Questions}
Before trying to answer them, let us first remember what the research questions of this thesis, which were reduce from $4$ to $3$ after running the pilot study:\\

\begin{enumerate}[RQ1.-]
        \item In a complex sentence, how does the mood alternation of the embedded verb that occurs due to the negation of the main or matrix verb affect the factuality value of the embedded event?\label{item:rq1}
        \item How does an individual subject affect the factuality judgment of the event?\label{item:rq3}
        \item How does a subject that refers to a collective entity like an institution, affect the factuality judgment of the event?\label{item:rq4}
\end{enumerate}

Based on the results obtained from the two studies we can now give clearer answers than the ones initially given in Section \ref{sect:pildisc}. First, the answer for questions \ref{item:rq4} and \ref{item:rq3} is quite clear: Having an individual subject vs. having a collective barely affects the factuality of the embedded event. In other words, the specificity of the subject in terms of the number of entities to which they refer in singular is non-veridical.\\

As to the first question the answer is slightly more complicated. From the results obtained in both studies we know that overall mood alternation affects the factuality of the embedded predicate, but this effect is not significant. It appears though that a more detailed analysis based on matrix verbs and probably even in other factors like the presence of modal verbs might show significant effects in a few specific cases. In other words, the veridicality of mood alternation is bigger than that of specificity as analyzed here, but it is still rather small. Finally let us consider what future lines of work can be developed based on what was presented in this thesis.\\

\section{Future Work}
Throughout these pages two studies have been presented and their analysis focus more on what the data looks like ($AC_2$, CLMMs, etc.) than on why does it look like that. Only a few possibilities, like the influence of world knowledge and the possible effect of modal verbs, that could explain the results have been given. Thus, a main line of future work would be a thourough analysis of the data in terms of explaining how the annotations look like.\\

A second line of work would follow what was mentioned above about inherent disagreement and scores that better reflect what the annotations look like. For example, it could be insightful to compare both kinds of analysis to understand the differences in the information yielded by each method. In any case, gathering more annotations is required.\\

The third and last line of work proposed is the inclusion of out-of-sentence context in the corpus. The question about its inclusion was already raised while designing the main study. This seemed reasonable given the results of the pilot study and that, as \citet{faulkner2021systematic} shows, both the presence and the informativity of the out-of-sentence-context influence the acceptability of mood alternation. Furthermore, since in Section \ref{subsect:man} it was demonstrated that the difficulties in reference resolution might have increased disagreement and uncertainty in the chosen labels, context could had solve this problem. But, given the results of \citet{pavlick2019inherent} where it was shown that disagreement increased with context, it is difficult to predict how agreement would have looked like if context had been added. Furthermore, as already mentioned, \citet{faulkner2021systematic} shows that the informativity of the context matters, thus, adding it would have complicated an experimental design already difficult enough. Nevertheless, given the lack of studies in NLI that include context, it is definitely a line of research that needs to be developed, specially since a pragmatic approach should include context, and sentence context is not enough to fulfiil this.\\